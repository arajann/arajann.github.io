<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Apply Machine Learning Methods to Predict Stroke</title>

<script src="site_libs/header-attrs-2.10/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="childhood-trauma-study.html">Childhood Trauma Study</a>
</li>
<li>
  <a href="ML-Methods-to-Predict-Stroke.html">ML Methods to Predict Stroke</a>
</li>
<li>
  <a href="RCT-Proposal.html">Randomized Control Trial Proposal</a>
</li>
<li>
  <a href="dashboard.html">Dashboard</a>
</li>
<li>
  <a href="mailto:&lt;anandrajan44@gmail.com&gt;">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/arajann/">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Apply Machine Learning Methods to Predict Stroke</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="datamotivation" class="section level2">
<h2>Data/Motivation</h2>
<p>For our project our team chose to use the stroke prediction dataset from <a href="https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?datasetId=1120859&amp;language=R">kaggle</a>. <a href="https://www.stroke.org/en/about-stroke#:~:text=Stroke%20is%20a%20disease%20that,or%20bursts%20(or%20ruptures)">Stroke</a> is a disease that affects the arteries leading to and within the brain. It is the No. 5 cause of death and a leading cause of disability in the United States. Our motivation was to use this dataset to help identify key indicators that lead to strokes, since many are preventable. The questions we are trying to answer using this dataset are: 1) What are the key indicators or risk factors that lead to stroke occurrence? 2) Which classification model performs best in predicting these key indicators?</p>
</div>
<div id="data-cleaning" class="section level2">
<h2>Data Cleaning</h2>
<p>To properly analyze the data and build models to make predictions, the first step is to clean the data. The vast majority of the data was already fairly clean and ready for analysis, but we began by reading in the data, cleaning variable names, making categorical data factors, and making continuous data of type numeric. We also decided to exclude the variable “id” since it will not be relevant to our analysis.</p>
<p>The resulting dataset contains 5110 patient records and 11 columns. The dependent variable is the binary variable “stroke” with response values “Yes” and “No”. Other variables include: gender, age, hypertension, heart disease, ever been married, work type, residence type, average glucose level, BMI and smoking status. A detailed list of all the variables in the dataset and their corresponding levels are available in the appendix.</p>
</div>
</div>
<div id="eda" class="section level1">
<h1>EDA</h1>
<p>To create a graphical summary of the numeric data, we used featurePlot. Note that to produce the density plots for this dataset we need to exclude the categorical variables, so they were removed. <img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>From the plots, we can see that the distribution of BMI and average glucose level for patients who experienced a stroke, versus patients who didn’t are fairly similar. However, the distribution of age across the two groups is fairly different. Patients who experienced a stroke tended to be of a higher age.</p>
<p>To visualize the categorical variables, we decided to create plots of the relative proportion of variable levels faceted by stroke occurrence. These plots help visualize any noticeable differences in the distribution of the categorical variables by stroke type.</p>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-4-1.png" width="672" /><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>From the visualizations, we can see that among those who experienced strokes: a noticeably higher proportion of individuals had hypertension, heart disease, have ever been married, have formerly smoked or smoke currently. These variables could be potential indicators of stroke occurrence in the models.</p>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<p>To begin the modeling process, we split the data into training (75%) and test (25%) data. Since all the remaining predictors in the dataset are potential risk factors for strokes, we decided to include them in all of the following models.</p>
<p>We can examine how many rows contain missing data in the training and test datasets.</p>
<p>There are 152 rows in the training data containing missing values, and there are 49 rows in the test data containing missing values.</p>
<div id="imputation" class="section level2">
<h2>Imputation</h2>
<p>We apply the k nearest neighbor (k = 5) method for imputation where we assume any missing data is missing at random.</p>
<p>The training data now contains 3833 rows. The test data now contains 1277 rows.</p>
</div>
<div id="model-fitting" class="section level2">
<h2>Model Fitting</h2>
<p>The resampling method we used for all of the models is as follows: k-fold cross validation, two class summary (since the ROC curve is only for two classes), and classProbs = T, since AUC/ROC is the evaluation criteria.</p>
</div>
<div id="penalized-logistic-regression" class="section level2">
<h2>Penalized Logistic Regression</h2>
<p>The first model we fit to the dataset was Penalized Logistic Regression. Some assumptions of logistic regression include requiring the observations to be independent of each other, little or no multicollinearity among independent variables, and linearity of independent variables and log odds. These assumptions are also limitations of the model.</p>
<p>The tuning parameter in the penalty term of the model controls its flexibility. We decided to test a grid of tuning parameters. For alpha, since we wanted to use the elastic net method, we created a sequence from 0 to 1. For lambda, we created a sequence of numbers to test different values. The model then selects optimal values from the grid which are: alpha = 0.45 and lambda = 0.0140853. To display this visually, we created a plot of the model AUC for various tuning parameters in the grid. This plot can be viewed in the appendix.</p>
<p>4 predictors are included in the final model, as seen when the coefficients are extracted.</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Final Model Coefficients</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-3.4650850</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="right">0.9015004</td>
</tr>
<tr class="odd">
<td align="left">hypertensionYes</td>
<td align="right">0.2912709</td>
</tr>
<tr class="even">
<td align="left">heart_diseaseYes</td>
<td align="right">0.4148165</td>
</tr>
<tr class="odd">
<td align="left">avg_glucose_level</td>
<td align="right">0.1725724</td>
</tr>
</tbody>
</table>
<p>These include age, having hypertension, having heart disease and average glucose level. This suggests they play important roles in predicting the response.</p>
</div>
<div id="mars" class="section level2">
<h2>MARS</h2>
<p>Next, we trained a MARS model on the imputed training data.</p>
<p>The MARS model has two tuning parameters: degree and nprune. Degree is the degree of interaction and nprune is the number of retained terms. We decided to test a grid of tuning parameters. The value of degree that maximizes the cross-validated AUC is 3, and the value of nprune that maximizes the cross-validated AUC is 5. The cross-validation plot can be viewed in the appendix.</p>
<p>The model that maximizes the cross-validated AUC can be viewed in the appendix.</p>
<p>A limitation of MARS models is that they can suffer from high variance.</p>
</div>
<div id="svm-with-linear-kernel" class="section level2">
<h2>SVM with Linear Kernel</h2>
<p>The next model we fit to the imputed training data was a Support Vector Machine with a Linear Kernel. An assumption of the model is that the data is linearly separable. This is also a limitation of the model, as they don’t perform well when this isn’t the case.</p>
<p>The model contains a tuning parameter C, also known as Cost, that determines the possible misclassifications. It essentially imposes a penalty to the model for making an error: the higher the value of C, the less likely it is that the SVM algorithm will misclassify a point. We decided to test a grid of tuning parameters for C. The model then selects the optimal value from the grid which maximizes the model AUC. That value is C = 0.0183156. To display this visually, we created a plot of the model AUC for various values of the tuning parameter in the grid. This plot can be viewed in the appendix.</p>
<p>The final SVM with a linear kernel that maximizes the cross-validated AUC needed 421 support vectors to fit the line. This model can be viewed in the appendix.</p>
</div>
<div id="svm-with-radial-kernel" class="section level2">
<h2>SVM with Radial Kernel</h2>
<p>We then trained a Support Vector Machine with a Radial Kernel on the imputed training data.</p>
<p>The model has two tuning parameters: C and sigma. C quantifies the cost of misclassification and sigma is related to the flexibility of the decision boundary. We decided to test a grid of tuning parameters. The values of C and sigma that maximize the cross-validated AUC are 0.411 and 3.4^{-4}.The cross-validation plot can be viewed in the appendix.</p>
<p>The final SVM with a radial kernel that maximizes the cross-validated AUC needed 387 support vectors to fit the line. This model can be viewed in the appendix.</p>
<p>A limitation of SVMs is the black box nature of these functions. The use of kernels to separate the data makes them difficult to interpret. SVMs also do not directly provide probability estimates.</p>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>For our final model, we trained a Random Forest model on the imputed training data.</p>
<p>The tuning parameters for the model are mtry, splitrule, and min.node.size. For the splitrule we specified “gini” which corresponds to the gini index, since this is a classification problem. We then decided to test a grid of tuning parameters for mtry and min.node.size. mtry is the number of randomly selected predictors at each cut in the tree. We created a grid from 1 to 10, where 10 is the number of predictors in the dataset. min.node.size controls the size of the tree, so we created a sequence of numbers to test different values. The parameter is the minimum number of observations in a terminal node. The model then selects optimal values from the grid which are: mtry = 1 and min.node.size = 2. The cross-validation plot can be viewed in the appendix.</p>
<p>A limitation of random forests is that it can overfit datasets that are particularly noisy. In addition, for data including categorical predictor variables with different number of levels, random forests are biased in favor of those predictors with more levels.</p>
</div>
<div id="comparing-models" class="section level2">
<h2>Comparing Models</h2>
<p>To decide which model we will use to predict the response variable, let’s use the AUC to compare model performance.</p>
<table>
<thead>
<tr class="header">
<th align="left">model</th>
<th align="right">AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Penalized Logistic Regression</td>
<td align="right">0.841</td>
</tr>
<tr class="even">
<td align="left">MARS</td>
<td align="right">0.834</td>
</tr>
<tr class="odd">
<td align="left">Random Forest</td>
<td align="right">0.823</td>
</tr>
<tr class="even">
<td align="left">SVM (Radial Kernel)</td>
<td align="right">0.731</td>
</tr>
<tr class="odd">
<td align="left">SVM (Linear Kernel)</td>
<td align="right">0.696</td>
</tr>
</tbody>
</table>
<p>The penalized logistic regression model has the highest cross-validated AUC and will be used as the final model.</p>
<p>Let’s get its AUC on the imputed test dataset and plot the ROC curve.</p>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>We will now make a Confusion Matrix</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">No</th>
<th align="right">Yes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">No</td>
<td align="right">1215</td>
<td align="right">62</td>
</tr>
<tr class="even">
<td align="left">Yes</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>From the confusion matrix we have a 95.1448708% accuracy, and thus a testing error rate of 4.86%. From the table, we can see that our model is not able to actually detect stroke cases at a 50% threshold. We should consult with experts on stroke to determine what threshold to use. The full confusion matrix is available in the appendix.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>The main takeaways from this report are that age, hypertension, heart disease and average glucose level each have a positive relationship with stroke. In considering the cross-validated model performance of penalized logistic regression, MARS, support vector machine with a linear kernel, support vector machine with a radial kernel, and random forest, penalized logistic regression has the highest cross-validated model performance. Additionally, for this model, the threshold needs to be adjusted, in consultation with experts on stroke, to detect stroke cases as the prevalence of stroke in this dataset is low. Individuals cannot control aging, but they can focus on diminishing other risk factors such as hypertension, heart disease and average glucose level, with methods such as a healthy diet, frequent exercise, and stress reduction. These results were also largely expected due to significant scientific research conducted on risk factors for strokes.</p>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="variables-and-levels-of-variables-in-dataset" class="section level3">
<h3>Variables and Levels of Variables in Dataset</h3>
<ol style="list-style-type: decimal">
<li>Gender (factor)</li>
</ol>
<ul>
<li>Female</li>
<li>Male</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Age (numeric)</li>
<li>Hypertension (factor)</li>
</ol>
<ul>
<li>Yes</li>
<li>No</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Heart disease (factor)</li>
</ol>
<ul>
<li>Yes</li>
<li>No</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Ever been married (factor)</li>
</ol>
<ul>
<li>Yes</li>
<li>No</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Work type (factor)</li>
</ol>
<ul>
<li>Private</li>
<li>Self-employed</li>
<li>Children</li>
<li>Government job</li>
<li>Never worked</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>Residence type (factor)</li>
</ol>
<ul>
<li>Yes</li>
<li>No</li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li>Average glucose level (numeric)</li>
<li>BMI (numeric)</li>
<li>Smoking status (factor)</li>
</ol>
<ul>
<li>Formerly smoked</li>
<li>Smokes</li>
<li>Never smoked</li>
<li>Unknown</li>
<li>Other</li>
</ul>
<ol start="11" style="list-style-type: decimal">
<li>Stroke (factor)</li>
</ol>
<ul>
<li>Yes</li>
<li>No</li>
</ul>
</div>
<div id="penalized-logistic-regression-cross-validation-plot" class="section level3">
<h3>Penalized Logistic Regression Cross-Validation Plot</h3>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="mars-cross-validation-plot" class="section level3">
<h3>MARS Cross-Validation Plot</h3>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="mars-final-model" class="section level3">
<h3>MARS Final Model</h3>
<table>
<colgroup>
<col width="87%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-3.8754171</td>
</tr>
<tr class="even">
<td align="left">h(age-0.11223) * h(avg_glucose_level- -0.239578)</td>
<td align="right">0.5714259</td>
</tr>
<tr class="odd">
<td align="left">h(age-0.11223) * hypertensionYes * h(-0.239578-avg_glucose_level)</td>
<td align="right">2.0946522</td>
</tr>
<tr class="even">
<td align="left">h(age-0.11223) * work_typeSelf-employed * h(avg_glucose_level- -0.239578)</td>
<td align="right">-0.3446909</td>
</tr>
<tr class="odd">
<td align="left">h(age-0.11223) * h(1.30143-bmi)</td>
<td align="right">0.7516836</td>
</tr>
</tbody>
</table>
</div>
<div id="svm-linear-kernel-cross-validation-plot" class="section level3">
<h3>SVM (Linear Kernel) Cross-Validation Plot</h3>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="svm-linear-kernel-final-model" class="section level3">
<h3>SVM (Linear Kernel) Final Model</h3>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 0.0183156388887342 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 421 
## 
## Objective Function Value : -6.85 
## Training error : 0.048787 
## Probability model included.</code></pre>
</div>
<div id="svm-radial-kernel-cross-validation-plot" class="section level3">
<h3>SVM (Radial Kernel) Cross-Validation Plot</h3>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="svm-radial-kernel-final-model" class="section level3">
<h3>SVM (Radial Kernel) Final Model</h3>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 0.411112290507187 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.000335462627902512 
## 
## Number of Support Vectors : 387 
## 
## Objective Function Value : -153.7554 
## Training error : 0.048787 
## Probability model included.</code></pre>
</div>
<div id="random-forest-cross-validation-plot" class="section level3">
<h3>Random Forest Cross-Validation Plot</h3>
<p><img src="ML-Methods-to-Predict-Stroke_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="penalized-logistic-regression-confusion-matrix" class="section level3">
<h3>Penalized Logistic Regression Confusion Matrix</h3>
<pre class="r"><code>test.pred.prob &lt;- predict(model.glmn, newdata = testData_pp[,1:10],
                          type = &quot;prob&quot;)[,2]
test.pred &lt;-  rep(&quot;No&quot;,length(test.pred.prob))
test.pred[test.pred.prob &gt; 0.5] &lt;- &quot;Yes&quot;
test.pred &lt;- factor(test.pred,c(&quot;No&quot;,&quot;Yes&quot;))

confusionMatrix(data = test.pred,
                reference = testData_pp$stroke,
                positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1215   62
##        Yes    0    0
##                                           
##                Accuracy : 0.9514          
##                  95% CI : (0.9382, 0.9626)
##     No Information Rate : 0.9514          
##     P-Value [Acc &gt; NIR] : 0.5337          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : 9.408e-15       
##                                           
##             Sensitivity : 0.00000         
##             Specificity : 1.00000         
##          Pos Pred Value :     NaN         
##          Neg Pred Value : 0.95145         
##              Prevalence : 0.04855         
##          Detection Rate : 0.00000         
##    Detection Prevalence : 0.00000         
##       Balanced Accuracy : 0.50000         
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
